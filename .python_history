print(3/4)
print(3/4*100)
print(b5/3)
print((int(3/4))
print(int(3/4))
'aa'.count()
'aa'.count('')
'aaff'.count('')
'aaff sds'.count('')
'aaff'.count('')
'aaff  '.count('')
'aaff  '.count(' ')
s = 'sfs  sdf sf'
s.count('')-s.count(' ')
s.count('')-s.count(' ')-1
a = ['a','a']
b=[ len(ans) for ans in a]
b
from llama import ModelArgs, Transformer, LLaMA                                                                                                                                                             
from pathlib import Path
import torch
import json
import os
from fairscale.nn.model_parallel.initialize import initialize_model_parallel
ckpt_dir = '/data/z0/heehoon/llama/llama_pretrained/30B'
world_size = 4
max_seq_len = 512
max_batch_size = 12
def load(
) -> LLaMA:
    torch.distributed.init_process_group("nccl")
    initialize_model_parallel(world_size)
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    if local_rank > 0:
        return 0
    checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
    assert world_size == len(
        checkpoints
    ), f"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}"
    ckpt_values = []
    for i in range(world_size):
        ckpt_path = checkpoints[i]
        print("Loading ", i)
        ckpt_values.append(torch.load(ckpt_path, map_location="cpu"))
    with open(Path(ckpt_dir) / "params.json", "r") as f:
        params = json.loads(f.read())
    model_args: ModelArgs = ModelArgs(
        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params
    )
    #torch.set_default_tensor_type(torch.cuda.HalfTensor)
    model_args.vocab_size = 32000
    print(model_args)
    model = Transformer(model_args)
    torch.set_default_tensor_type(torch.FloatTensor)
    for i in range(world_size):
        model.load_state_dict(ckpt_values[i], strict=False)
        print(i)
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    if local_rank > 0:
        sys.stdout = open(os.devnull, "w")
    print(model)
    for name, param in model.named_parameters():
        print(name, param.shape)
    generator = LLaMA(model)
    return generator
from llama import ModelArgs, Transformer, LLaMA                                                                                                                                                             
from pathlib import Path
import torch
import json
import os
from fairscale.nn.model_parallel.initialize import initialize_model_parallel
ckpt_dir = '/data/z0/heehoon/llama/llama_pretrained/30B'
world_size = 4
max_seq_len = 512
max_batch_size = 12
def load(
) -> LLaMA:
    torch.distributed.init_process_group("nccl")
    initialize_model_parallel(world_size)
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    if local_rank > 0:
        return 0
    checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
    assert world_size == len(
        checkpoints
    ), f"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}"
    ckpt_values = []
    for i in range(world_size):
        ckpt_path = checkpoints[i]
        print("Loading ", i)
        ckpt_values.append(torch.load(ckpt_path, map_location="cpu"))
    with open(Path(ckpt_dir) / "params.json", "r") as f:
        params = json.loads(f.read())
    model_args: ModelArgs = ModelArgs(
        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params
    )
    #torch.set_default_tensor_type(torch.cuda.HalfTensor)
    model_args.vocab_size = 32000
    print(model_args)
    model = Transformer(model_args)
    torch.set_default_tensor_type(torch.FloatTensor)
    for i in range(world_size):
        model.load_state_dict(ckpt_values[i], strict=False)
        print(i)
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    if local_rank > 0:
        sys.stdout = open(os.devnull, "w")
    print(model)
    for name, param in model.named_parameters():
        print(name, param.shape)
    generator = LLaMA(model)
    return generator
ls
import torch
ckpt_dir = '/data/z0/heehoon/llama/llama_pretrained/30B'
world_size = 4
max_seq_len = 513
max_seq_len = 512
max_batch_size = 12
from pathlib import Path
checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
ckpt_values = []
for i in range(world_size):
	ckpt_path= checkpoints[i]
	print(i)
	ckpt_values.append(torch.load(ckpt_path, map_location="cpu"))
dir(checkpoints[i])
checkpoints[i].name
checkpoints[0].name
checkpoints[0].__module__
ckpt_values[i]
ckpt_values[0].keys()
for k, v in ckpt_values[0]:
	print(v.shape)
for k, v in ckpt_values[0].items():
	print(v.shape)
	print(k, v.shape)
for k, v in ckpt_values[0].items():
	print(k, v.shape)
ckpt_values[0]['layers.0.attention_norm.weight']
ckpt_values[1]
def merge_params(name):
	end0 = ['wq.weight', 'wk.weight', 'wv.weight', 'w1.weight', 'w3.weight']
	end1 = ['wo.weight', 'w2.weight']
	start0 = ['output']
	start1 = ['tok_embeddings']
	if name[:6] in start0 or name[-9:] in end0:
		for i in range(4):
			return torch.cat([a[name] for a in ckpt_values], dim=0)
	if name[:14] in start1 or name[-9:] in end1:
		for i in range(4):
			return torch.cat([a[name] for a in ckpt_values], dim=1)
	return ckpt_values[0][name]
for k in ckpt_values[0].keys():
merged = {}
for k in ckpt_values[0].keys():
	merged[k] = merge_params(name)
for k in ckpt_values[0].keys():
	merged[k] = merge_params(k)
for k,v in merged.items():
	print(k, v.shape)
start = [ 'layers.'+str(i) for i in range(15)]
starat
start
start.append('tok')
start
for k,v in merged.items():
	
model = {}
for k,v in merged.items():
	if any([k.startswith(st) for st in starat]):
		model[k] = v

	if any([k.startswith(st) for st in start]):
		model[k] = v
model
start
start = [ 'layers.'+str(i) for i in range(6,15)]

start.extend([ 'layers.'+str(i) +'.' for i in range(6)])
start
start.append('tok')
for k,v in merged.items():
	if any([k.startswith(st) for st in start]):
		model[k] = v
for k,v in model.items():
	print(k,v.shape)
model = {}
for k,v in model.items():
start.extend([ 'layers.'+str(i) +'.' for i in range(6)])
for k,v in merged.items():
	if any([k.startswith(st) for st in start]):
		model[k] = v
for k,v in model.items():
	print(k,v.shape)
torch.save(model, './param0.pth')
model = {}
for k,v in merged.items():
model = {}
start = [ 'layers.'+str(i) for i in range(15,30)]
start
for k,v in merged.items():
	if any([k.startswith(st) for st in start]):
		model[k] = v
for k,v in merged.items():
for k,v in model.items():
	print(k,v.shape)
torch.save(model, './param1.pth')
start = [ 'layers.'+str(i) for i in range(30,45)]
start
model = {}
for k,v in merged.items():
	if any([k.startswith(st) for st in start]):
		model[k] = v
for k,v in model.items():
	print(k,v.shape)
torch.save(model, './param2.pth')
start = [ 'layers.'+str(i) for i in range(45,60)]
start.append('norm')
start.append('output')
start
model = {}
for k,v in merged.items():
	if any([k.startswith(st) for st in start]):
		model[k] = v
for k,v in model.items():
	print(k,v.shape)
torch.save(model, './param3.pth')
%hisotyr
%hisotry
%history
import readline
for i in range(readline.get_current_history_length()):
    print (readline.get_history_item(i + 1))
